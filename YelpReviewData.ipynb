{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "from collections import Counter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-04-09 23:53:07.320384\n",
      "4153150\n"
     ]
    }
   ],
   "source": [
    "t1 = datetime.now()\n",
    "s = \"\"\n",
    "with open(\"yelp_dataset_challenge_round9/yelp_academic_dataset_review.json\") as f:\n",
    "    reviews = f.read().strip().split(\"\\n\")\n",
    "print(datetime.now())\n",
    "print(len(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"review_id\":\"NxL8SIC5yqOdnlXCg18IBg\",\"user_id\":\"KpkOkG6RIf4Ra25Lhhxf1A\",\"business_id\":\"2aFiy99vNLklCx3T_tGS9A\",\"stars\":5,\"date\":\"2011-10-10\",\"text\":\"If you enjoy service by someone who is as competent as he is personable, I would recommend Corey Kaplan highly. The time he has spent here has been very productive and working with him educational and enjoyable. I hope not to need him again (though this is highly unlikely) but knowing he is there if I do is very nice. By the way, I'm not from El Centro, CA. but Scottsdale, AZ.\",\"useful\":0,\"funny\":0,\"cool\":0,\"type\":\"review\"}\n",
      "\n",
      "{\"review_id\":\"pXbbIgOXvLuTi_SPs1hQEQ\",\"user_id\":\"bQ7fQq1otn9hKX-gXRsrgA\",\"business_id\":\"2aFiy99vNLklCx3T_tGS9A\",\"stars\":5,\"date\":\"2010-12-29\",\"text\":\"After being on the phone with Verizon Wireless trying to figure out why my phone wasn't working for 4.5 hours, I was put in touch with Sharpie Tech. Well, after 10 seconds they fixed the problem. As the owner of a company that does our best numbers over the holiday season, having my phone out of order for 4.5 hours was horrible. Sharpie Tech fixed the problem in record time, even Verizon was shocked. I highly recommend working with this company and I look forward to working with them more. \\n\\n-Rachel Charlupski\\nFounder and CEO, The Babysitting Company\",\"useful\":1,\"funny\":0,\"cool\":0,\"type\":\"review\"}\n",
      "\n",
      "{\"review_id\":\"ry7vldTxIOQfOfqgYoS0Fw\",\"user_id\":\"hTth3h8I_p6Gg6jpPeQeMQ\",\"business_id\":\"-tLxryf1OpzVP9OSrznprg\",\"stars\":2,\"date\":\"2013-11-07\",\"text\":\"I saw Dr. Makoto Trotter a few times, and although I think he's very personable, I wouldn't recommend going there for actual treatment. He didn't seem very knowledgeable, and ultimately didn't help me one bit with any of my issues. Other reviewers of Makoto have said that he seemed more like a salesman, and I agree. He makes you quite uncomfortable by constantly pushing to sell his supplements. Often assuming that you are already buying it as he is \\\"recommending\\\" his product. I blew hundreds of dollars on useless tests and supplements, and that's after declining a whole bunch of other useless products that he \\\"recommended\\\" (i.e., adds it to your bill as you walk out unless you specifically decline).\",\"useful\":1,\"funny\":0,\"cool\":0,\"type\":\"review\"}\n"
     ]
    }
   ],
   "source": [
    "#Print the first review\n",
    "print(reviews[0])\n",
    "print(\"\\n\"+reviews[1])\n",
    "print(\"\\n\"+reviews[10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-787748fbcd81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfirst_review\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msecond_review\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnth_review\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alexiscandelaria/anaconda/lib/python2.7/json/__init__.pyc\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alexiscandelaria/anaconda/lib/python2.7/json/decoder.pyc\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \"\"\"\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import json\n",
    "first_review = json.loads(reviews[0])\n",
    "second_review = json.loads(reviews[1])\n",
    "nth_review = json.loads(reviews[10000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "review_text = first_review['text']\n",
    "review_text2 = second_review['text']\n",
    "review_text3 = nth_review['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'If you enjoy service by someone who is as competent as he is personable, I would recommend Corey Kaplan highly.',\n",
       " u'The time he has spent here has been very productive and working with him educational and enjoyable.',\n",
       " u'I hope not to need him again (though this is highly unlikely) but knowing he is there if I do is very nice.',\n",
       " u\"By the way, I'm not from El Centro, CA.\",\n",
       " u'but Scottsdale, AZ.']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sent_tokenize(review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "tokenized_text = word_tokenize(review_text)\n",
    "#remove stop words\n",
    "english_stops = set(stopwords.words('english'))\n",
    "#text after stop words are removed\n",
    "words = [word for word in tokenized_text if word not in english_stops]\n",
    "#print(words)\n",
    "#turns out that we probably do not want to remove stop words, as neutrality is measured in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import subjectivity\n",
    "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:100]]\n",
    "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:100]]\n",
    "#print(subj_docs)\n",
    "#print(obj_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "\n",
    "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:100]]\n",
    "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:100]]\n",
    "\n",
    "#We separately split subjective and objective instances to keep a balanced uniform class distribution in both train and test sets.\n",
    "\n",
    "train_subj_docs = subj_docs[:80]\n",
    "test_subj_docs = subj_docs[80:100]\n",
    "train_obj_docs = obj_docs[:80]\n",
    "test_obj_docs = obj_docs[80:100]\n",
    "training_docs = train_subj_docs+train_obj_docs\n",
    "testing_docs = test_subj_docs+test_obj_docs\n",
    "sentim_analyzer = SentimentAnalyzer()\n",
    "all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])\n",
    "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
    "\n",
    "#print(all_words_neg)\n",
    "#print(\"\\n!!!\")\n",
    "#print(unigram_feats)\n",
    "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)\n",
    "# will most likely not use this but just use the vader sentinment Intensity analysis instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n",
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy: 0.8\n",
      "F-measure [obj]: 0.8\n",
      "F-measure [subj]: 0.8\n",
      "Precision [obj]: 0.8\n",
      "Precision [subj]: 0.8\n",
      "Recall [obj]: 0.8\n",
      "Recall [subj]: 0.8\n"
     ]
    }
   ],
   "source": [
    "training_set = sentim_analyzer.apply_features(training_docs)\n",
    "test_set = sentim_analyzer.apply_features(testing_docs)\n",
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentim_analyzer.train(trainer, training_set)\n",
    "for key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
    "    print('{0}: {1}'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you enjoy service by someone who is as competent as he is personable, I would recommend Corey Kaplan highly. The time he has spent here has been very productive and working with him educational and enjoyable. I hope not to need him again (though this is highly unlikely) but knowing he is there if I do is very nice. By the way, I'm not from El Centro, CA. but Scottsdale, AZ.\n",
      "\n",
      "Actual rating: 5\n",
      "compound:0.8895 neg:0.0 neu:0.823 pos:0.177 \n",
      "\n",
      "\n",
      "After being on the phone with Verizon Wireless trying to figure out why my phone wasn't working for 4.5 hours, I was put in touch with Sharpie Tech. Well, after 10 seconds they fixed the problem. As the owner of a company that does our best numbers over the holiday season, having my phone out of order for 4.5 hours was horrible. Sharpie Tech fixed the problem in record time, even Verizon was shocked. I highly recommend working with this company and I look forward to working with them more. \n",
      "\n",
      "-Rachel Charlupski\n",
      "Founder and CEO, The Babysitting Company\n",
      "\n",
      "Actual rating: 5\n",
      "compound:0.1513 neg:0.103 neu:0.789 pos:0.108 \n",
      "\n",
      "\n",
      "I saw Dr. Makoto Trotter a few times, and although I think he's very personable, I wouldn't recommend going there for actual treatment. He didn't seem very knowledgeable, and ultimately didn't help me one bit with any of my issues. Other reviewers of Makoto have said that he seemed more like a salesman, and I agree. He makes you quite uncomfortable by constantly pushing to sell his supplements. Often assuming that you are already buying it as he is \"recommending\" his product. I blew hundreds of dollars on useless tests and supplements, and that's after declining a whole bunch of other useless products that he \"recommended\" (i.e., adds it to your bill as you walk out unless you specifically decline).\n",
      "\n",
      "Actual rating: 2\n",
      "compound:-0.7803 neg:0.106 neu:0.851 pos:0.043 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Intensity Analyzer Using Vader\n",
    "from __future__ import print_function\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import tokenize\n",
    "reviews = [first_review, second_review, nth_review]\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "for review in reviews:\n",
    "    review_text = review['text']\n",
    "    rating = review['stars']\n",
    "    ss = sid.polarity_scores(review_text)\n",
    "    print(review_text + \"\\n\\nActual rating: {}\".format(rating))\n",
    "    for k in sorted(ss):\n",
    "        print(\"{0}:{1} \".format(k, ss[k]),end='')\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
